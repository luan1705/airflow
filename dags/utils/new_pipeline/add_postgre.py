from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError, OperationalError
from utils.new_pipeline.merge import tintuc
import pandas as pd
import logging
import time
from datetime import datetime
import traceback

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
)
log = logging.getLogger(__name__)

def create_db_connection(max_retries=3):
    """
    T·∫°o k·∫øt n·ªëi database v·ªõi retry logic
    """
    connection_string = 'postgresql://vnsfintech:%40Vns123456@videv.cloud:5432/vnsfintech'
    
    for attempt in range(max_retries):
        try:
            log.info(f"üîå Th·ª≠ k·∫øt n·ªëi database (l·∫ßn {attempt + 1}/{max_retries})")
            
            enginedb = create_engine(
                connection_string,
                connect_args={
                    'connect_timeout': 15,
                    'application_name': 'airflow_news_scraper'
                },
                pool_pre_ping=True,  # Verify connections before use
                pool_recycle=3600,   # Recycle connections every hour
                echo=False
            )
            
            # Test connection
            with enginedb.connect() as conn:
                result = conn.execute(text("SELECT 1"))
                result.fetchone()
            
            log.info("‚úÖ K·∫øt n·ªëi PostgreSQL th√†nh c√¥ng")
            return enginedb
            
        except OperationalError as e:
            log.warning(f"‚ö†Ô∏è K·∫øt n·ªëi th·∫•t b·∫°i l·∫ßn {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                log.info(f"‚è≥ Ch·ªù {wait_time}s tr∆∞·ªõc khi th·ª≠ l·∫°i...")
                time.sleep(wait_time)
        except Exception as e:
            log.error(f"‚ùå L·ªói kh√¥ng mong mu·ªën: {e}")
            break
    
    raise ConnectionError("Kh√¥ng th·ªÉ k·∫øt n·ªëi database sau nhi·ªÅu l·∫ßn th·ª≠")

def validate_dataframe(df, required_columns=['Titles', 'Link', 'Source']):
    """
    Validate DataFrame structure v√† data quality
    """
    if df is None:
        return False, "DataFrame is None"
    
    if df.empty:
        return False, "DataFrame is empty"
    
    # Check required columns
    missing_cols = [col for col in required_columns if col not in df.columns]
    if missing_cols:
        return False, f"Missing columns: {missing_cols}"
    
    # Check for null values in critical columns
    critical_nulls = df[['Titles', 'Link']].isnull().sum()
    if critical_nulls.any():
        log.warning(f"‚ö†Ô∏è Null values found: {critical_nulls.to_dict()}")
        # Remove rows with null Titles or Link
        initial_count = len(df)
        df_clean = df.dropna(subset=['Titles', 'Link'])
        if len(df_clean) != initial_count:
            log.info(f"üßπ Removed {initial_count - len(df_clean)} rows with null Titles/Link")
    
    # Check for empty strings
    empty_titles = (df['Titles'].str.strip() == '').sum()
    empty_links = (df['Link'].str.strip() == '').sum()
    
    if empty_titles > 0 or empty_links > 0:
        log.warning(f"‚ö†Ô∏è Empty strings - Titles: {empty_titles}, Links: {empty_links}")
    
    return True, "Valid DataFrame"

def add_postgres():
    """
    Main function ƒë·ªÉ scrape v√† l∆∞u v√†o PostgreSQL
    """
    start_time = datetime.now()
    enginedb = None
    
    try:
        log.info("üöÄ B·∫Øt ƒë·∫ßu add_postgres task")
        log.info(f"üïí Start time: {start_time}")
        
        # 1. L·∫•y d·ªØ li·ªáu t·ª´ scrapers
        log.info("üì• ƒêang l·∫•y d·ªØ li·ªáu t·ª´ tintuc()...")
        df = tintuc()
        
        # 2. Validate d·ªØ li·ªáu
        is_valid, message = validate_dataframe(df)
        if not is_valid:
            log.info(f"üì≠ {message}. Task v·∫´n SUCCESS.")
            return f"No data: {message}"
        
        log.info(f"üìä Received {len(df)} articles from scrapers")
        log.info(f"üìã Columns: {df.columns.tolist()}")
        
        # 3. K·∫øt n·ªëi database
        enginedb = create_db_connection()
        
        # 4. L·∫•y d·ªØ li·ªáu hi·ªán t·∫°i ƒë·ªÉ check tr√πng l·∫∑p
        log.info("üîç ƒêang ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p...")
        
        try:
            # Th√™m LIMIT ƒë·ªÉ tr√°nh load qu√° nhi·ªÅu data n·∫øu b·∫£ng l·ªõn
            df_existing = pd.read_sql(
                'SELECT "Titles", "Link" FROM "news"."News" ORDER BY "Date" DESC LIMIT 10000',
                enginedb
            )
            log.info(f"üóÉÔ∏è Loaded {len(df_existing)} existing records for comparison")
            
        except Exception as e:
            log.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load existing data: {e}")
            # N·∫øu b·∫£ng kh√¥ng t·ªìn t·∫°i ho·∫∑c l·ªói, coi nh∆∞ kh√¥ng c√≥ d·ªØ li·ªáu c≈©
            df_existing = pd.DataFrame(columns=['Titles', 'Link'])
        
        # 5. L·ªçc tr√πng l·∫∑p
        if not df_existing.empty:
            # Merge ƒë·ªÉ t√¨m records m·ªõi
            df_merged = df.merge(
                df_existing, 
                on=['Titles', 'Link'], 
                how='left', 
                indicator=True
            )
            df_new = df_merged[df_merged['_merge'] == 'left_only'].drop(columns=['_merge'])
            
            duplicates_count = len(df) - len(df_new)
            log.info(f"üîç Found {duplicates_count} duplicates, {len(df_new)} new records")
        else:
            df_new = df
            log.info("üÜï No existing data, all records are new")
        
        # 6. Ki·ªÉm tra c√≥ d·ªØ li·ªáu m·ªõi kh√¥ng
        if df_new.empty:
            log.info("üì≠ Kh√¥ng c√≥ b·∫£n ghi m·ªõi ƒë·ªÉ th√™m v√†o b·∫£ng News.")
            return "Kh√¥ng c√≥ b·∫£n ghi m·ªõi"
        
        # 7. Data cleaning tr∆∞·ªõc khi insert
        log.info("üßπ Cleaning data tr∆∞·ªõc khi insert...")
        
        # Remove duplicates within new data
        initial_new_count = len(df_new)
        df_new = df_new.drop_duplicates(subset=['Titles', 'Link'], keep='first')
        internal_dups = initial_new_count - len(df_new)
        if internal_dups > 0:
            log.info(f"üîÑ Removed {internal_dups} internal duplicates")
        
        # Clean strings
        df_new['Titles'] = df_new['Titles'].str.strip()
        df_new['Link'] = df_new['Link'].str.strip()
        
        # 8. Insert v√†o database
        log.info(f"üíæ ƒêang insert {len(df_new)} records v√†o database...")
        
        try:
            rows_affected = df_new.to_sql(
                name='News',
                schema='news',
                con=enginedb,
                if_exists='append',
                index=False,
                method='multi'  # Faster batch inserts
            )
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            log.info(f"‚úÖ Successfully inserted {len(df_new)} records v√†o DB trong {duration:.2f}s")
            
            # Log summary statistics
            if 'Source' in df_new.columns:
                source_stats = df_new['Source'].value_counts().to_dict()
                log.info(f"üìä Records by source: {source_stats}")
            
            return f"Successfully added {len(df_new)} new records to database"
            
        except Exception as e:
            log.error(f"‚ùå Database insert failed: {e}")
            log.error(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
            raise
    
    except ConnectionError as e:
        log.error(f"‚ùå Database connection error: {e}")
        raise
    
    except Exception as e:
        log.error(f"‚ùå Unexpected error in add_postgres: {e}")
        log.error(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
        raise
    
    finally:
        # Cleanup database connection
        if enginedb:
            try:
                enginedb.dispose()
                log.info("üîå Database connection disposed")
            except:
                pass

